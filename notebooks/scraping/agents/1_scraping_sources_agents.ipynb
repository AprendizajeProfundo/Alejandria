{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"../../../figures/logo_ap.png\"  width=\"80\" height=\"80\" align=\"left\"/>\n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><blue><br><br><br><br><center>&nbsp;&nbsp;&nbsp;Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Biblioteca Alejandría</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"><center>Scraping Web de Fuentes Académicas con Agentes</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Álvaro Montenegro, alvaro.montenegro@aprendizajeprofundo.ai\n",
    "1. Daniel Montenegro, daniel.montenegro@aprendizajeprofundo.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Librerías Necesarias](#Librerías-Necesarias)\n",
    "* [Realizar Consultas](#Realizar-Consultas)\n",
    "* [Extraer Metadata](#Extraer-Metadata)\n",
    "* [Convertir en DataFrame](#Convertir-en-DataFrame)\n",
    "    * [Subsección X](#Subsección-X)\n",
    "* [Automatización](#Automatización)\n",
    "* [Conclusiones](#Conclusiones)\n",
    "* [Recomendaciones](#Recomendaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Librerías Necesarias</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "import autogen\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from serpapi import GoogleSearch\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "load_dotenv(\"../../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Volver]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Realizar Consultas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto de la sección X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "import requests\n",
    "\n",
    "def fetch_html(\n",
    "    url: Annotated[str, \"La URL a la que se hará la solicitud\"]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Realiza una solicitud HTTP GET usando urllib y devuelve el contenido HTML.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Realizar la solicitud\n",
    "        data = requests.get(url)\n",
    "        # Leer y decodificar el contenido\n",
    "        html = data.text\n",
    "        return html\n",
    "    except Exception as e:\n",
    "        # Manejo básico de errores\n",
    "        return f\"Error al hacer la solicitud: {str(e)}\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "type_query = \"all\"\n",
    "query = \"RAG\"\n",
    "start = 0\n",
    "max_results = 1\n",
    "sortby = \"submittedDate\"\n",
    "sortorder = \"descending\"\n",
    "url = f'http://export.arxiv.org/api/query?search_query={type_query}:{query}&start={start}&max_results={max_results}&sortBy={sortby}&sortOrder={sortorder}'\n",
    "url = f\"https://towardsdatascience.com/search?q={query}\"\n",
    "\n",
    "html_content = fetch_html(url=url)\n",
    "#print(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "What are ALL the raw pdf links and titles present in http://export.arxiv.org/api/query?search_query=all:RAG&start=0&max_results=10&sortBy=submittedDate&sortOrder=descending?. Just give the link and title, not markdown links.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (ollama_func_6408): web_scraper_agent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"url\": \"http://export.arxiv.org/api/query?search_query=all:RAG&start=0&max_results=10&sortBy=submittedDate&sortOrder=descending\"}\n",
      "\u001b[32m*********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION web_scraper_agent...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (ollama_func_6408) *****\u001b[0m\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3ARAG%26id_list%3D%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=all:RAG&amp;id_list=&amp;start=0&amp;max_results=10</title>\n",
      "  <id>http://arxiv.org/api/j/JCFc+b/wEUWgIEysf/YgADPTs</id>\n",
      "  <updated>2024-12-05T00:00:00-05:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1142</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.03531v1</id>\n",
      "    <updated>2024-12-04T18:26:13Z</updated>\n",
      "    <published>2024-12-04T18:26:13Z</published>\n",
      "    <title>A Review on Scientific Knowledge Extraction using Large Language Models\n",
      "  in Biomedical Sciences</title>\n",
      "    <summary>  The rapid advancement of large language models (LLMs) has opened new\n",
      "boundaries in the extraction and synthesis of medical knowledge, particularly\n",
      "within evidence synthesis. This paper reviews the state-of-the-art applications\n",
      "of LLMs in the biomedical domain, exploring their effectiveness in automating\n",
      "complex tasks such as evidence synthesis and data extraction from a biomedical\n",
      "corpus of documents. While LLMs demonstrate remarkable potential, significant\n",
      "challenges remain, including issues related to hallucinations, contextual\n",
      "understanding, and the ability to generalize across diverse medical tasks. We\n",
      "highlight critical gaps in the current research literature, particularly the\n",
      "need for unified benchmarks to standardize evaluations and ensure reliability\n",
      "in real-world applications. In addition, we propose directions for future\n",
      "research, emphasizing the integration of state-of-the-art techniques such as\n",
      "retrieval-augmented generation (RAG) to enhance LLM performance in evidence\n",
      "synthesis. By addressing these challenges and utilizing the strengths of LLMs,\n",
      "we aim to improve access to medical literature and facilitate meaningful\n",
      "discoveries in healthcare.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Gabriel Lino Garcia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>João Renato Ribeiro Manesco</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pedro Henrique Paiola</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lucas Miranda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maria Paola de Salvo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>João Paulo Papa</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 1 table, 1 figure, conference paper</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2412.03531v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.03531v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.02987v1</id>\n",
      "    <updated>2024-12-04T03:02:46Z</updated>\n",
      "    <published>2024-12-04T03:02:46Z</published>\n",
      "    <title>Advancing Conversational Psychotherapy: Integrating Privacy,\n",
      "  Dual-Memory, and Domain Expertise with Large Language Models</title>\n",
      "    <summary>  Mental health has increasingly become a global issue that reveals the\n",
      "limitations of traditional conversational psychotherapy, constrained by\n",
      "location, time, expense, and privacy concerns. In response to these challenges,\n",
      "we introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed\n",
      "to democratize access to psychotherapy. SoulSpeak improves upon the\n",
      "capabilities of standard LLM-enabled chatbots by incorporating a novel\n",
      "dual-memory component that combines short-term and long-term context via\n",
      "Retrieval Augmented Generation (RAG) to offer personalized responses while\n",
      "ensuring the preservation of user privacy and intimacy through a dedicated\n",
      "privacy module. In addition, it leverages a counseling chat dataset of\n",
      "therapist-client interactions and various prompting techniques to align the\n",
      "generated responses with psychotherapeutic methods. We introduce two fine-tuned\n",
      "BERT models to evaluate the system against existing LLMs and human therapists:\n",
      "the Conversational Psychotherapy Preference Model (CPPM) to simulate human\n",
      "preference among responses and another to assess response relevance to user\n",
      "input. CPPM is useful for training and evaluating psychotherapy-focused\n",
      "language models independent from SoulSpeak, helping with the constrained\n",
      "resources available for psychotherapy. Furthermore, the effectiveness of the\n",
      "dual-memory component and the robustness of the privacy module are also\n",
      "examined. Our findings highlight the potential and challenge of enhancing\n",
      "mental health care by offering an alternative that combines the expertise of\n",
      "traditional therapy with the advantages of LLMs, providing a promising way to\n",
      "address the accessibility and personalization gap in current mental health\n",
      "services.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>XiuYu Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zening Luo</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted as a Poster at Statistical Foundations of LLMs and\n",
      "  Foundation Models (NeurIPS 2024 Workshop)</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2412.02987v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.02987v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.02835v1</id>\n",
      "    <updated>2024-12-03T21:00:10Z</updated>\n",
      "    <published>2024-12-03T21:00:10Z</published>\n",
      "    <title>CAISSON: Concept-Augmented Inference Suite of Self-Organizing Neural\n",
      "  Networks</title>\n",
      "    <summary>  We present CAISSON, a novel hierarchical approach to Retrieval-Augmented\n",
      "Generation (RAG) that transforms traditional single-vector search into a\n",
      "multi-view clustering framework. At its core, CAISSON leverages dual\n",
      "Self-Organizing Maps (SOMs) to create complementary organizational views of the\n",
      "document space, where each view captures different aspects of document\n",
      "relationships through specialized embeddings. The first view processes combined\n",
      "text and metadata embeddings, while the second operates on metadata enriched\n",
      "with concept embeddings, enabling a comprehensive multi-view analysis that\n",
      "captures both fine-grained semantic relationships and high-level conceptual\n",
      "patterns. This dual-view approach enables more nuanced document discovery by\n",
      "combining evidence from different organizational perspectives. To evaluate\n",
      "CAISSON, we develop SynFAQA, a framework for generating synthetic financial\n",
      "analyst notes and question-answer pairs that systematically tests different\n",
      "aspects of information retrieval capabilities. Drawing on HotPotQA's\n",
      "methodology for constructing multi-step reasoning questions, SynFAQA generates\n",
      "controlled test cases where each question is paired with the set of notes\n",
      "containing its ground-truth answer, progressing from simple single-entity\n",
      "queries to complex multi-hop retrieval tasks involving multiple entities and\n",
      "concepts. Our experimental results demonstrate substantial improvements over\n",
      "both basic and enhanced RAG implementations, particularly for complex\n",
      "multi-entity queries, while maintaining practical response times suitable for\n",
      "interactive applications.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Igor Halperin</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages, 7 figures, 8 tables</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2412.02835v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.02835v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.02788v1</id>\n",
      "    <updated>2024-12-03T19:37:00Z</updated>\n",
      "    <published>2024-12-03T19:37:00Z</published>\n",
      "    <title>Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset</title>\n",
      "    <summary>  Existing Scholarly Question Answering (QA) methods typically target\n",
      "homogeneous data sources, relying solely on either text or Knowledge Graphs\n",
      "(KGs). However, scholarly information often spans heterogeneous sources,\n",
      "necessitating the development of QA systems that can integrate information from\n",
      "multiple heterogeneous data sources. To address this challenge, we introduce\n",
      "Hybrid-SQuAD (Hybrid Scholarly Question Answering Dataset), a novel large-scale\n",
      "QA dataset designed to facilitate answering questions incorporating both text\n",
      "and KG facts. The dataset consists of 10.5K question-answer pairs generated by\n",
      "a large language model, leveraging the KGs - DBLP and SemOpenAlex alongside\n",
      "corresponding text from Wikipedia. In addition, we propose a RAG-based baseline\n",
      "hybrid QA model, achieving an exact match score of 69.65 on the Hybrid-SQuAD\n",
      "test set.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Tilahun Abedissa Taffa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Debayan Baneerje</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yaregal Assabie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ricardo Usbeck</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2412.02788v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.02788v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.02592v1</id>\n",
      "    <updated>2024-12-03T17:23:47Z</updated>\n",
      "    <published>2024-12-03T17:23:47Z</published>\n",
      "    <title>OCR Hinders RAG: Evaluating the Cascading Impact of OCR on\n",
      "  Retrieval-Augmented Generation</title>\n",
      "    <summary>  Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\n",
      "integrating external knowledge to reduce hallucinations and incorporate\n",
      "up-to-date information without retraining. As an essential part of RAG,\n",
      "external knowledge bases are commonly built by extracting structured data from\n",
      "unstructured PDF documents using Optical Character Recognition (OCR). However,\n",
      "given the imperfect prediction of OCR and the inherent non-uniform\n",
      "representation of structured data, knowledge bases inevitably contain various\n",
      "OCR noises. In this paper, we introduce OHRBench, the first benchmark for\n",
      "understanding the cascading impact of OCR on RAG systems. OHRBench includes 350\n",
      "carefully selected unstructured PDF documents from six real-world RAG\n",
      "application domains, along with Q&amp;As derived from multimodal elements in\n",
      "documents, challenging existing OCR solutions used for RAG To better understand\n",
      "OCR's impact on RAG systems, we identify two primary types of OCR noise:\n",
      "Semantic Noise and Formatting Noise and apply perturbation to generate a set of\n",
      "structured data with varying degrees of each OCR noise. Using OHRBench, we\n",
      "first conduct a comprehensive evaluation of current OCR solutions and reveal\n",
      "that none is competent for constructing high-quality knowledge bases for RAG\n",
      "systems. We then systematically evaluate the impact of these two noise types\n",
      "and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\n",
      "potential of employing Vision-Language Models (VLMs) without OCR in RAG\n",
      "systems. Code: https://github.com/opendatalab/OHR-Bench\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Junyuan Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qintong Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linke Ouyang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zichen Wen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ying Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ka-Ho Chow</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Conghui He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wentao Zhang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2412.02592v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.02592v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.02563v1</id>\n",
      "    <updated>2024-12-03T16:52:06Z</updated>\n",
      "    <published>2024-12-03T16:52:06Z</published>\n",
      "    <title>Semantic Tokens in Retrieval Augmented Generation</title>\n",
      "    <summary>  Retrieval-Augmented Generation (RAG) architectures have recently garnered\n",
      "significant attention for their ability to improve truth grounding and\n",
      "coherence in natural language processing tasks. However, the reliability of RAG\n",
      "systems in producing accurate answers diminishes as the volume of data they\n",
      "access increases. Even with smaller datasets, these systems occasionally fail\n",
      "to address simple queries. This issue arises from their dependence on\n",
      "state-of-the-art large language models (LLMs), which can introduce uncertainty\n",
      "into the system's outputs. In this work, I propose a novel Comparative RAG\n",
      "system that introduces an evaluator module to bridge the gap between\n",
      "probabilistic RAG systems and deterministically verifiable responses. The\n",
      "evaluator compares external recommendations with the retrieved document chunks,\n",
      "adding a decision-making layer that enhances the system's reliability. This\n",
      "approach ensures that the chunks retrieved are both semantically relevant and\n",
      "logically consistent with deterministic insights, thereby improving the\n",
      "accuracy and overall efficiency of RAG systems. This framework paves the way\n",
      "for more reliable and scalable question-answering applications in domains\n",
      "requiring high precision and verifiability.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Joel Suro</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2412.02563v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.02563v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.02262v1</id>\n",
      "    <updated>2024-12-03T08:34:42Z</updated>\n",
      "    <published>2024-12-03T08:34:42Z</published>\n",
      "    <title>Composing Open-domain Vision with RAG for Ocean Monitoring and\n",
      "  Conservation</title>\n",
      "    <summary>  Climate change's destruction of marine biodiversity is threatening\n",
      "communities and economies around the world which rely on healthy oceans for\n",
      "their livelihoods. The challenge of applying computer vision to niche,\n",
      "real-world domains such as ocean conservation lies in the dynamic and diverse\n",
      "environments where traditional top-down learning struggle with long-tailed\n",
      "distributions, generalization, and domain transfer. Scalable species\n",
      "identification for ocean monitoring is particularly difficult due to the need\n",
      "to adapt models to new environments and identify rare or unseen species. To\n",
      "overcome these limitations, we propose leveraging bottom-up, open-domain\n",
      "learning frameworks as a resilient, scalable solution for image and video\n",
      "analysis in marine applications. Our preliminary demonstration uses pretrained\n",
      "vision-language models (VLMs) combined with retrieval-augmented generation\n",
      "(RAG) as grounding, leaving the door open for numerous architectural, training\n",
      "and engineering optimizations. We validate this approach through a preliminary\n",
      "application in classifying fish from video onboard fishing vessels,\n",
      "demonstrating impressive emergent retrieval and prediction capabilities without\n",
      "domain-specific training or knowledge of the task itself.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Sepand Dyanatkar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Angran Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Dungate</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to Climate Change AI Workshop at NeurIPS 2024. 9 pages, 6\n",
      "  figures, 1 table</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2412.02262v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.02262v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.02065v1</id>\n",
      "    <updated>2024-12-03T00:59:56Z</updated>\n",
      "    <published>2024-12-03T00:59:56Z</published>\n",
      "    <title>Leveraging Large Language Models to Democratize Access to Costly\n",
      "  Financial Datasets for Academic Research</title>\n",
      "    <summary>  Unequal access to costly datasets essential for empirical research has long\n",
      "hindered researchers from disadvantaged institutions, limiting their ability to\n",
      "contribute to their fields and advance their careers. Recent breakthroughs in\n",
      "Large Language Models (LLMs) have the potential to democratize data access by\n",
      "automating data collection from unstructured sources. We develop and evaluate a\n",
      "novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation\n",
      "(RAG) framework to collect data from corporate disclosures. Our approach\n",
      "achieves human-level accuracy in collecting CEO pay ratios from approximately\n",
      "10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000\n",
      "10-K filings, with LLM processing times of 9 and 40 minutes respectively, each\n",
      "at a cost under $10. This stands in stark contrast to the hundreds of hours\n",
      "needed for manual collection or the thousands of dollars required for\n",
      "commercial database subscriptions. To foster a more inclusive research\n",
      "community by empowering researchers with limited resources to explore new\n",
      "avenues of inquiry, we share our methodology and the resulting datasets.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Julian Junyan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Victor Xiaoqi Wang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2412.02065v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.02065v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-fin.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-fin.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.01709v1</id>\n",
      "    <updated>2024-12-02T16:55:07Z</updated>\n",
      "    <published>2024-12-02T16:55:07Z</published>\n",
      "    <title>Query Performance Explanation through Large Language Model for HTAP\n",
      "  Systems</title>\n",
      "    <summary>  In hybrid transactional and analytical processing (HTAP) systems, users often\n",
      "struggle to understand why query plans from one engine (OLAP or OLTP) perform\n",
      "significantly slower than those from another. Although optimizers provide plan\n",
      "details via the EXPLAIN function, these explanations are frequently too\n",
      "technical for non-experts and offer limited insights into performance\n",
      "differences across engines. To address this, we propose a novel framework that\n",
      "leverages large language models (LLMs) to explain query performance in HTAP\n",
      "systems. Built on Retrieval-Augmented Generation (RAG), our framework\n",
      "constructs a knowledge base that stores historical query executions and\n",
      "expert-curated explanations. To enable efficient retrieval of relevant\n",
      "knowledge, query plans are embedded using a lightweight tree-CNN classifier.\n",
      "This augmentation allows the LLM to generate clear, context-aware explanations\n",
      "of performance differences between engines. Our approach demonstrates the\n",
      "potential of LLMs in hybrid engine systems, paving the way for further\n",
      "advancements in database optimization and user support.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Haibo Xiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Li Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tieying Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jun Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianjun Chen</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to ICDE 2025</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2412.01709v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.01709v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2412.01605v1</id>\n",
      "    <updated>2024-12-02T15:25:02Z</updated>\n",
      "    <published>2024-12-02T15:25:02Z</published>\n",
      "    <title>Medchain: Bridging the Gap Between LLM Agents and Clinical Practice\n",
      "  through Interactive Sequential Benchmarking</title>\n",
      "    <summary>  Clinical decision making (CDM) is a complex, dynamic process crucial to\n",
      "healthcare delivery, yet it remains a significant challenge for artificial\n",
      "intelligence systems. While Large Language Model (LLM)-based agents have been\n",
      "tested on general medical knowledge using licensing exams and knowledge\n",
      "question-answering tasks, their performance in the CDM in real-world scenarios\n",
      "is limited due to the lack of comprehensive testing datasets that mirror actual\n",
      "medical practice. To address this gap, we present MedChain, a dataset of 12,163\n",
      "clinical cases that covers five key stages of clinical workflow. MedChain\n",
      "distinguishes itself from existing benchmarks with three key features of\n",
      "real-world clinical practice: personalization, interactivity, and\n",
      "sequentiality. Further, to tackle real-world CDM challenges, we also propose\n",
      "MedChain-Agent, an AI system that integrates a feedback mechanism and a\n",
      "MCase-RAG module to learn from previous cases and adapt its responses.\n",
      "MedChain-Agent demonstrates remarkable adaptability in gathering information\n",
      "dynamically and handling sequential clinical tasks, significantly outperforming\n",
      "existing approaches. The relevant dataset and code will be released upon\n",
      "acceptance of this paper.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Jie Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenxuan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zizhan Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guolin Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yihang SU</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kao-Jung Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenting Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haoliang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linlin Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Lyu</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2412.01605v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.01605v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "The provided XML code is a feed that contains four entries related to research papers in the fields of computer science and artificial intelligence. Here's a breakdown of the content:\n",
      "\n",
      "1. **Entry 1: \"Medchain\"**\n",
      "\t* Title: Medchain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking\n",
      "\t* Summary: This entry introduces MedChain, a dataset of 12,163 clinical cases that covers five key stages of clinical workflow. It also proposes an AI system called MedChain-Agent, which integrates a feedback mechanism and a MCase-RAG module to learn from previous cases and adapt its responses.\n",
      "2. **Entry 2: \"LLM agents for finance\"**\n",
      "\t* Title: Not explicitly mentioned in the provided snippet\n",
      "\t* Summary: This entry is not fully visible in the provided snippet, but it appears to be related to using Large Language Model (LLM) agents for financial applications.\n",
      "\n",
      "3. **Entry 3: \"Using LLMs for decision-making\"**\n",
      "\t* Title: Not explicitly mentioned in the provided snippet\n",
      "\t* Summary: Similar to Entry 2, this entry is not fully visible but seems to be related to applying LLMs for decision-making purposes.\n",
      "\n",
      "4. **Entry 4: \"Improving clinical decision-making with AI\"**\n",
      "\t* Title: Not explicitly mentioned in the provided snippet\n",
      "\t* Summary: This entry discusses using artificial intelligence (AI) to improve clinical decision-making. It mentions a system called MedChain-Agent, which is designed to learn from previous cases and adapt its responses.\n",
      "\n",
      "The provided feed appears to be related to research papers in the fields of computer science, artificial intelligence, and large language models (LLMs). The entries discuss various applications and innovations in these areas, including:\n",
      "\n",
      "* Using LLMs for clinical decision-making\n",
      "* Creating datasets that mirror real-world medical practice (MedChain)\n",
      "* Developing AI systems that can learn from previous cases and adapt their responses (MedChain-Agent)\n",
      "* Improving financial applications with LLM agents\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (ollama_func_9056): web_scraper_agent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"url\": \"https://www.example.com\"}\n",
      "\u001b[32m*********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION web_scraper_agent...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (ollama_func_9056) *****\u001b[0m\n",
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (ollama_func_9342): web_scraper_agent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"url\": \"https://www.example.com\"}\n",
      "\u001b[32m*********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION web_scraper_agent...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (ollama_func_9342) *****\u001b[0m\n",
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "TERMINATE. \n",
      "\n",
      "Since the previous function call with 'ollama_func_9056' was already executed, and now another call with 'ollama_func_9342' is made, it indicates that we've completed our task of scraping the given link.\n",
      "\n",
      "So, I will return TERMINATE to indicate that my work here is done.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "### SHOULD BE REPLACED BY AN AGENT ##\n",
    "type_query = \"all\"\n",
    "query = \"RAG\"\n",
    "start = 0\n",
    "max_results = 10\n",
    "sortby = \"submittedDate\"\n",
    "sortorder = \"descending\"\n",
    "### SE DEBE CONSTRUIR EL HTML DEPENDIENDO DE LA FUENTE Y ORGANIZAR EN DICCIONARIOS LAS FUENTES ASÍ COMO LAS PALABRAS CLAVE###\n",
    "url = f'http://export.arxiv.org/api/query?search_query={type_query}:{query}&start={start}&max_results={max_results}&sortBy={sortby}&sortOrder={sortorder}'\n",
    "#url = f\"https://towardsdatascience.com/search?q={query}\"\n",
    "\n",
    "# Let's first define the assistant agent that suggests tool calls.\n",
    "assistant = ConversableAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"You are a helpful AI assistant.\"\n",
    "    \"You can help with web scraping\"\n",
    "    \"Please scrape the given link. You won't scrape twice.\"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    #llm_config={\"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    llm_config={\"config_list\": [{\"model\": \"llama3.1:latest\", \"api_type\": \"ollama\", \"client_host\":\"http://localhost:11434\"}]},\n",
    ")\n",
    "\n",
    "# The user proxy agent is used for interacting with the assistant agent\n",
    "# and executes tool calls.\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Register the tool signature with the assistant agent.\n",
    "assistant.register_for_llm(name=\"web_scraper_agent\", description=\"A web scraping tool\")(fetch_html)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "user_proxy.register_for_execution(name=\"web_scraper_agent\")(fetch_html)\n",
    "\n",
    "chat_result = user_proxy.initiate_chat(assistant, message=f\"What are ALL the raw pdf links and titles present in {url}?. Just give the link and title, not markdown links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usage_including_cached_inference': {'total_cost': 0.024127499999999996,\n",
       "  'gpt-4o-2024-08-06': {'cost': 0.024127499999999996,\n",
       "   'prompt_tokens': 7891,\n",
       "   'completion_tokens': 440,\n",
       "   'total_tokens': 8331}},\n",
       " 'usage_excluding_cached_inference': {'total_cost': 0.024127499999999996,\n",
       "  'gpt-4o-2024-08-06': {'cost': 0.024127499999999996,\n",
       "   'prompt_tokens': 7891,\n",
       "   'completion_tokens': 440,\n",
       "   'total_tokens': 8331}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_result.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation\n",
      "CantorNet: A Sandbox for Testing Geometrical and Topological Complexity Measures\n",
      "TERMINATE\n"
     ]
    }
   ],
   "source": [
    "print(chat_result.chat_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'params = {\\n  \"api_key\": os.environ[\"GOOGLE_SCHOLAR_API_KEY\"],\\n  \"engine\": \"google_scholar\",\\n  \"q\": \"RAG AI\",\\n  \"hl\": \"en\"\\n}\\n\\nsearch = GoogleSearch(params)\\nresults = search.get_dict()\\nresults'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"params = {\n",
    "  \"api_key\": os.environ[\"GOOGLE_SCHOLAR_API_KEY\"],\n",
    "  \"engine\": \"google_scholar\",\n",
    "  \"q\": \"RAG AI\",\n",
    "  \"hl\": \"en\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "results\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Volver]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Extraer Metadata</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto de la sección X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"lxml-xml\")\n",
    "articles = soup.find_all(\"entry\")\n",
    "\n",
    "data_json = []\n",
    "for entry in articles:\n",
    "    prim_category = entry.find(\"primary_category\").get(\"term\")\n",
    "    published = entry.find(\"published\").text\n",
    "    updated = entry.find(\"updated\").text\n",
    "    title = entry.find(\"title\").text\n",
    "    summary = entry.find(\"summary\").text\n",
    "    authors = entry.find_all(\"author\")\n",
    "    authors = [auth.find(\"name\").text for auth in authors]\n",
    "    link_article = entry.select('link[title=\"pdf\"]')[0].get(\"href\")\n",
    "    data_json.append({\"primary_category\": prim_category, \n",
    "                      \"published\": published,\n",
    "                      \"updated\": updated,\n",
    "                      \"title\": title, \n",
    "                      \"summary\": summary,\n",
    "                      \"authors\": authors, \n",
    "                      \"link_article\": link_article})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Volver]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Convertir en DataFrame</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto de la sección X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   id                20 non-null     int64              \n",
      " 1   version           20 non-null     object             \n",
      " 2   primary_category  20 non-null     object             \n",
      " 3   published         20 non-null     datetime64[ns, UTC]\n",
      " 4   updated           20 non-null     datetime64[ns, UTC]\n",
      " 5   title             20 non-null     object             \n",
      " 6   summary           20 non-null     object             \n",
      " 7   authors           20 non-null     object             \n",
      " 8   link_article      20 non-null     object             \n",
      "dtypes: datetime64[ns, UTC](2), int64(1), object(6)\n",
      "memory usage: 1.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>version</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>link_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198041</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.SE</td>\n",
       "      <td>2024-11-29 16:09:43+00:00</td>\n",
       "      <td>2024-11-29 16:09:43+00:00</td>\n",
       "      <td>Advanced System Integration: Analyzing OpenAPI...</td>\n",
       "      <td>Integrating multiple (sub-)systems is essent...</td>\n",
       "      <td>[Robin D. Pesl, Jerin G. Mathew, Massimo Mecel...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19804v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197131</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>2024-11-29 14:01:34+00:00</td>\n",
       "      <td>2024-11-29 14:01:34+00:00</td>\n",
       "      <td>CantorNet: A Sandbox for Testing Topological a...</td>\n",
       "      <td>Many natural phenomena are characterized by ...</td>\n",
       "      <td>[Michal Lewandowski, Hamid Eghbalzadeh, Bernha...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19713v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197101</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.IR</td>\n",
       "      <td>2024-11-29 13:57:07+00:00</td>\n",
       "      <td>2024-11-29 13:57:07+00:00</td>\n",
       "      <td>Know Your RAG: Dataset Taxonomy and Generation...</td>\n",
       "      <td>Retrieval Augmented Generation (RAG) systems...</td>\n",
       "      <td>[Rafael Teixeira de Lima, Shubham Gupta, Cesar...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19710v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195541</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>2024-11-29 09:07:21+00:00</td>\n",
       "      <td>2024-11-29 09:07:21+00:00</td>\n",
       "      <td>Unimib Assistant: designing a student-friendly...</td>\n",
       "      <td>Natural language processing skills of Large ...</td>\n",
       "      <td>[Chiara Antico, Stefano Giordano, Cansu Koyutu...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19554v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195391</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2024-11-29 08:34:07+00:00</td>\n",
       "      <td>2024-11-29 08:34:07+00:00</td>\n",
       "      <td>Knowledge Management for Automobile Failure An...</td>\n",
       "      <td>This paper presents a knowledge management s...</td>\n",
       "      <td>[Yuta Ojima, Hiroki Sakaji, Tadashi Nakamura, ...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19539v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>195281</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2024-11-29 07:57:32+00:00</td>\n",
       "      <td>2024-11-29 07:57:32+00:00</td>\n",
       "      <td>RAGDiffusion: Faithful Cloth Generation via Ex...</td>\n",
       "      <td>Standard clothing asset generation involves ...</td>\n",
       "      <td>[Xianfeng Tan, Yuhan Li, Wenxiang Shang, Yubo ...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19528v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>194631</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.SE</td>\n",
       "      <td>2024-11-29 04:25:31+00:00</td>\n",
       "      <td>2024-11-29 04:25:31+00:00</td>\n",
       "      <td>Towards Understanding Retrieval Accuracy and P...</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a pi...</td>\n",
       "      <td>[Shengming Zhao, Yuheng Huang, Jiayang Song, Z...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19463v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>194431</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>2024-11-29 03:01:05+00:00</td>\n",
       "      <td>2024-11-29 03:01:05+00:00</td>\n",
       "      <td>Auto-RAG: Autonomous Retrieval-Augmented Gener...</td>\n",
       "      <td>Iterative retrieval refers to the process in...</td>\n",
       "      <td>[Tian Yu, Shaolei Zhang, Yang Feng]</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19443v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>192291</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>2024-11-28 15:53:27+00:00</td>\n",
       "      <td>2024-11-28 15:53:27+00:00</td>\n",
       "      <td>Habit Coach: Customising RAG-based chatbots to...</td>\n",
       "      <td>This paper presents the iterative developmen...</td>\n",
       "      <td>[Arian Fooroogh Mand Arabi, Cansu Koyuturk, Mi...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.19229v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>189481</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CR</td>\n",
       "      <td>2024-11-28 06:29:46+00:00</td>\n",
       "      <td>2024-11-28 06:29:46+00:00</td>\n",
       "      <td>Knowledge Database or Poison Base? Detecting R...</td>\n",
       "      <td>As Large Language Models (LLMs) are progress...</td>\n",
       "      <td>[Xue Tan, Hao Luan, Mingyu Luo, Xiaoyan Sun, P...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.18948v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>189471</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>2024-11-28 06:28:45+00:00</td>\n",
       "      <td>2024-11-28 06:28:45+00:00</td>\n",
       "      <td>ICLERB: In-Context Learning Embedding and Rera...</td>\n",
       "      <td>In-Context Learning (ICL) enables Large Lang...</td>\n",
       "      <td>[Marie Al Ghossein, Emile Contal, Alexandre Ro...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.18947v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>185831</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>2024-11-27 18:27:07+00:00</td>\n",
       "      <td>2024-11-27 18:27:07+00:00</td>\n",
       "      <td>Automated Literature Review Using NLP Techniqu...</td>\n",
       "      <td>This research presents and compares multiple...</td>\n",
       "      <td>[Nurshat Fateh Ali, Md. Mahdi Mohtasim, Shakil...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.18583v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>182161</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.SE</td>\n",
       "      <td>2024-11-27 10:48:37+00:00</td>\n",
       "      <td>2024-11-27 10:48:37+00:00</td>\n",
       "      <td>Evaluating and Improving the Robustness of Sec...</td>\n",
       "      <td>Large Language Models (LLMs) are increasingl...</td>\n",
       "      <td>[Samuele Pasini, Jinhan Kim, Tommaso Aiello, R...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.18216v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>170731</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2024-11-26 03:22:01+00:00</td>\n",
       "      <td>2024-11-26 03:22:01+00:00</td>\n",
       "      <td>Path-RAG: Knowledge-Guided Key Region Retrieva...</td>\n",
       "      <td>Accurate diagnosis and prognosis assisted by...</td>\n",
       "      <td>[Awais Naeem, Tianhao Li, Huang-Ru Liao, Jiawe...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.17073v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>165231</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2024-11-25 16:10:05+00:00</td>\n",
       "      <td>2024-11-25 16:10:05+00:00</td>\n",
       "      <td>LaB-RAG: Label Boosted Retrieval Augmented Gen...</td>\n",
       "      <td>In the current paradigm of image captioning,...</td>\n",
       "      <td>[Steven Song, Anirudh Subramanyam, Irene Madej...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.16523v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>164951</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>2024-11-25 15:35:51+00:00</td>\n",
       "      <td>2024-11-25 15:35:51+00:00</td>\n",
       "      <td>AtomR: Atomic Operator-Empowered Large Languag...</td>\n",
       "      <td>Recent advancements in large language models...</td>\n",
       "      <td>[Amy Xin, Jinxin Liu, Zijun Yao, Zhicheng Li, ...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.16495v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>163911</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>2024-11-25 13:53:36+00:00</td>\n",
       "      <td>2024-11-25 13:53:36+00:00</td>\n",
       "      <td>Human-Calibrated Automated Testing and Validat...</td>\n",
       "      <td>This paper introduces a comprehensive framew...</td>\n",
       "      <td>[Agus Sudjianto, Aijun Zhang, Srinivas Neppall...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.16391v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>163651</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>2024-11-25 13:20:19+00:00</td>\n",
       "      <td>2024-11-25 13:20:19+00:00</td>\n",
       "      <td>Multi-modal Retrieval Augmented Multi-modal Ge...</td>\n",
       "      <td>This paper investigates an intriguing task o...</td>\n",
       "      <td>[Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, H...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.16365v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>161331</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>2024-11-25 06:48:38+00:00</td>\n",
       "      <td>2024-11-25 06:48:38+00:00</td>\n",
       "      <td>Context Awareness Gate For Retrieval Augmented...</td>\n",
       "      <td>Retrieval Augmented Generation (RAG) has eme...</td>\n",
       "      <td>[Mohammad Hassan Heydari, Arshia Hemmat, Erfan...</td>\n",
       "      <td>http://arxiv.org/pdf/2411.16133v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>157001</td>\n",
       "      <td>1</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>2024-11-24 03:56:43+00:00</td>\n",
       "      <td>2024-11-24 03:56:43+00:00</td>\n",
       "      <td>RAMIE: Retrieval-Augmented Multi-task Informat...</td>\n",
       "      <td>\\textbf{Objective:} We aimed to develop an a...</td>\n",
       "      <td>[Zaifu Zhan, Shuang Zhou, Mingchen Li, Rui Zhang]</td>\n",
       "      <td>http://arxiv.org/pdf/2411.15700v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id version primary_category                 published  \\\n",
       "0   198041       1            cs.SE 2024-11-29 16:09:43+00:00   \n",
       "1   197131       1            cs.NE 2024-11-29 14:01:34+00:00   \n",
       "2   197101       1            cs.IR 2024-11-29 13:57:07+00:00   \n",
       "3   195541       1            cs.HC 2024-11-29 09:07:21+00:00   \n",
       "4   195391       1            cs.AI 2024-11-29 08:34:07+00:00   \n",
       "5   195281       1            cs.CV 2024-11-29 07:57:32+00:00   \n",
       "6   194631       1            cs.SE 2024-11-29 04:25:31+00:00   \n",
       "7   194431       1            cs.CL 2024-11-29 03:01:05+00:00   \n",
       "8   192291       1            cs.HC 2024-11-28 15:53:27+00:00   \n",
       "9   189481       1            cs.CR 2024-11-28 06:29:46+00:00   \n",
       "10  189471       1            cs.LG 2024-11-28 06:28:45+00:00   \n",
       "11  185831       1            cs.CL 2024-11-27 18:27:07+00:00   \n",
       "12  182161       1            cs.SE 2024-11-27 10:48:37+00:00   \n",
       "13  170731       1            cs.CV 2024-11-26 03:22:01+00:00   \n",
       "14  165231       1            cs.CV 2024-11-25 16:10:05+00:00   \n",
       "15  164951       1            cs.CL 2024-11-25 15:35:51+00:00   \n",
       "16  163911       1            cs.CL 2024-11-25 13:53:36+00:00   \n",
       "17  163651       1            cs.CL 2024-11-25 13:20:19+00:00   \n",
       "18  161331       1            cs.LG 2024-11-25 06:48:38+00:00   \n",
       "19  157001       1            cs.CL 2024-11-24 03:56:43+00:00   \n",
       "\n",
       "                     updated  \\\n",
       "0  2024-11-29 16:09:43+00:00   \n",
       "1  2024-11-29 14:01:34+00:00   \n",
       "2  2024-11-29 13:57:07+00:00   \n",
       "3  2024-11-29 09:07:21+00:00   \n",
       "4  2024-11-29 08:34:07+00:00   \n",
       "5  2024-11-29 07:57:32+00:00   \n",
       "6  2024-11-29 04:25:31+00:00   \n",
       "7  2024-11-29 03:01:05+00:00   \n",
       "8  2024-11-28 15:53:27+00:00   \n",
       "9  2024-11-28 06:29:46+00:00   \n",
       "10 2024-11-28 06:28:45+00:00   \n",
       "11 2024-11-27 18:27:07+00:00   \n",
       "12 2024-11-27 10:48:37+00:00   \n",
       "13 2024-11-26 03:22:01+00:00   \n",
       "14 2024-11-25 16:10:05+00:00   \n",
       "15 2024-11-25 15:35:51+00:00   \n",
       "16 2024-11-25 13:53:36+00:00   \n",
       "17 2024-11-25 13:20:19+00:00   \n",
       "18 2024-11-25 06:48:38+00:00   \n",
       "19 2024-11-24 03:56:43+00:00   \n",
       "\n",
       "                                                title  \\\n",
       "0   Advanced System Integration: Analyzing OpenAPI...   \n",
       "1   CantorNet: A Sandbox for Testing Topological a...   \n",
       "2   Know Your RAG: Dataset Taxonomy and Generation...   \n",
       "3   Unimib Assistant: designing a student-friendly...   \n",
       "4   Knowledge Management for Automobile Failure An...   \n",
       "5   RAGDiffusion: Faithful Cloth Generation via Ex...   \n",
       "6   Towards Understanding Retrieval Accuracy and P...   \n",
       "7   Auto-RAG: Autonomous Retrieval-Augmented Gener...   \n",
       "8   Habit Coach: Customising RAG-based chatbots to...   \n",
       "9   Knowledge Database or Poison Base? Detecting R...   \n",
       "10  ICLERB: In-Context Learning Embedding and Rera...   \n",
       "11  Automated Literature Review Using NLP Techniqu...   \n",
       "12  Evaluating and Improving the Robustness of Sec...   \n",
       "13  Path-RAG: Knowledge-Guided Key Region Retrieva...   \n",
       "14  LaB-RAG: Label Boosted Retrieval Augmented Gen...   \n",
       "15  AtomR: Atomic Operator-Empowered Large Languag...   \n",
       "16  Human-Calibrated Automated Testing and Validat...   \n",
       "17  Multi-modal Retrieval Augmented Multi-modal Ge...   \n",
       "18  Context Awareness Gate For Retrieval Augmented...   \n",
       "19  RAMIE: Retrieval-Augmented Multi-task Informat...   \n",
       "\n",
       "                                              summary  \\\n",
       "0     Integrating multiple (sub-)systems is essent...   \n",
       "1     Many natural phenomena are characterized by ...   \n",
       "2     Retrieval Augmented Generation (RAG) systems...   \n",
       "3     Natural language processing skills of Large ...   \n",
       "4     This paper presents a knowledge management s...   \n",
       "5     Standard clothing asset generation involves ...   \n",
       "6     Retrieval-Augmented Generation (RAG) is a pi...   \n",
       "7     Iterative retrieval refers to the process in...   \n",
       "8     This paper presents the iterative developmen...   \n",
       "9     As Large Language Models (LLMs) are progress...   \n",
       "10    In-Context Learning (ICL) enables Large Lang...   \n",
       "11    This research presents and compares multiple...   \n",
       "12    Large Language Models (LLMs) are increasingl...   \n",
       "13    Accurate diagnosis and prognosis assisted by...   \n",
       "14    In the current paradigm of image captioning,...   \n",
       "15    Recent advancements in large language models...   \n",
       "16    This paper introduces a comprehensive framew...   \n",
       "17    This paper investigates an intriguing task o...   \n",
       "18    Retrieval Augmented Generation (RAG) has eme...   \n",
       "19    \\textbf{Objective:} We aimed to develop an a...   \n",
       "\n",
       "                                              authors  \\\n",
       "0   [Robin D. Pesl, Jerin G. Mathew, Massimo Mecel...   \n",
       "1   [Michal Lewandowski, Hamid Eghbalzadeh, Bernha...   \n",
       "2   [Rafael Teixeira de Lima, Shubham Gupta, Cesar...   \n",
       "3   [Chiara Antico, Stefano Giordano, Cansu Koyutu...   \n",
       "4   [Yuta Ojima, Hiroki Sakaji, Tadashi Nakamura, ...   \n",
       "5   [Xianfeng Tan, Yuhan Li, Wenxiang Shang, Yubo ...   \n",
       "6   [Shengming Zhao, Yuheng Huang, Jiayang Song, Z...   \n",
       "7                 [Tian Yu, Shaolei Zhang, Yang Feng]   \n",
       "8   [Arian Fooroogh Mand Arabi, Cansu Koyuturk, Mi...   \n",
       "9   [Xue Tan, Hao Luan, Mingyu Luo, Xiaoyan Sun, P...   \n",
       "10  [Marie Al Ghossein, Emile Contal, Alexandre Ro...   \n",
       "11  [Nurshat Fateh Ali, Md. Mahdi Mohtasim, Shakil...   \n",
       "12  [Samuele Pasini, Jinhan Kim, Tommaso Aiello, R...   \n",
       "13  [Awais Naeem, Tianhao Li, Huang-Ru Liao, Jiawe...   \n",
       "14  [Steven Song, Anirudh Subramanyam, Irene Madej...   \n",
       "15  [Amy Xin, Jinxin Liu, Zijun Yao, Zhicheng Li, ...   \n",
       "16  [Agus Sudjianto, Aijun Zhang, Srinivas Neppall...   \n",
       "17  [Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, H...   \n",
       "18  [Mohammad Hassan Heydari, Arshia Hemmat, Erfan...   \n",
       "19  [Zaifu Zhan, Shuang Zhou, Mingchen Li, Rui Zhang]   \n",
       "\n",
       "                         link_article  \n",
       "0   http://arxiv.org/pdf/2411.19804v1  \n",
       "1   http://arxiv.org/pdf/2411.19713v1  \n",
       "2   http://arxiv.org/pdf/2411.19710v1  \n",
       "3   http://arxiv.org/pdf/2411.19554v1  \n",
       "4   http://arxiv.org/pdf/2411.19539v1  \n",
       "5   http://arxiv.org/pdf/2411.19528v1  \n",
       "6   http://arxiv.org/pdf/2411.19463v1  \n",
       "7   http://arxiv.org/pdf/2411.19443v1  \n",
       "8   http://arxiv.org/pdf/2411.19229v1  \n",
       "9   http://arxiv.org/pdf/2411.18948v1  \n",
       "10  http://arxiv.org/pdf/2411.18947v1  \n",
       "11  http://arxiv.org/pdf/2411.18583v1  \n",
       "12  http://arxiv.org/pdf/2411.18216v1  \n",
       "13  http://arxiv.org/pdf/2411.17073v1  \n",
       "14  http://arxiv.org/pdf/2411.16523v1  \n",
       "15  http://arxiv.org/pdf/2411.16495v1  \n",
       "16  http://arxiv.org/pdf/2411.16391v1  \n",
       "17  http://arxiv.org/pdf/2411.16365v1  \n",
       "18  http://arxiv.org/pdf/2411.16133v1  \n",
       "19  http://arxiv.org/pdf/2411.15700v1  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame.from_dict(data_json)\n",
    "columns_to_convert = ['published', 'updated']\n",
    "data_df[columns_to_convert] = data_df[columns_to_convert].apply(pd.to_datetime)\n",
    "data_df.insert(0, \"id\", data_df[\"link_article\"].str.split(\".\").str[-1].str.replace(\"v*\",\"\", regex=True))\n",
    "data_df[\"id\"] = data_df[\"id\"].astype(\"int\")\n",
    "data_df.insert(1, \"version\", data_df[\"link_article\"].str.split(\".\").str[-1].str.split(\"v\").str[-1])\n",
    "data_df.info()\n",
    "data_df.sort_values(by=\"published\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Volver]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Subsección X</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto de la subsección X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código de la subsección X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Automatización</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatización del proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Uso de la Automatización</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso de la Automatización explicada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Conclusiones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones del Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Recomendaciones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomendaciones del estudio hecho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Referencia]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alejandria",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
