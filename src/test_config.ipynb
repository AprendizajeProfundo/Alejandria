{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93dd187-80dd-449b-a715-c338890709dd",
   "metadata": {},
   "source": [
    "### Parámetros de Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2333eaa-df55-4ad4-a732-d7a5cc315fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías Usadas\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Configuración para Arxiv API (se usa el endpoint de consulta con búsqueda)\n",
    "ARXIV_API_URL = \"http://export.arxiv.org/api/query?search_query={type_query}:{query}&start={start}&max_results={max_results}&sortBy={sortby}&sortOrder={sortorder}\"\n",
    "\n",
    "# Configuración para GitHub API\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\", \"\")\n",
    "\n",
    "# Parámetro por defecto para la consulta en Arxiv (por ejemplo, \"RAG\")\n",
    "QUERY_TOPIC = \"RAG\"\n",
    "TYPE_QUERY = \"all\"\n",
    "START=0\n",
    "MAX_RES=10\n",
    "SORTBY = \"submittedDate\"\n",
    "SORTORDER = \"descending\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc54015-b9bd-4dbc-ada8-a5cc73f1720d",
   "metadata": {},
   "source": [
    "### Consulta a la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9f97f19-bc2f-4816-9ab5-23dcda006549",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ARXIV_API_URL.format(type_query=TYPE_QUERY,query=QUERY_TOPIC,start=START,max_results=MAX_RES,sortby=SORTBY,sortorder=SORTORDER)\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c774b-ca41-4397-8dd9-d28c2d10909c",
   "metadata": {},
   "source": [
    "### Parser de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46f4c513-26cd-40e0-b459-55820d6ac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(xml_data):\n",
    "        root = ET.fromstring(xml_data)\n",
    "        ns = {\"atom\": \"http://www.w3.org/2005/Atom\", \"arxiv\": \"http://arxiv.org/schemas/atom\"}\n",
    "        articles = []\n",
    "        for entry in root.findall(\"atom:entry\", ns):\n",
    "            # Versión: se extrae del id (p.ej. ...v1, ...v2, etc.)\n",
    "            version = None\n",
    "            m = re.search(r'v(\\d+)$', id_text)\n",
    "            if m:\n",
    "                version = m.group(1)\n",
    "            # Categoría primaria\n",
    "            primary_category = \"\"\n",
    "            primary_cat_elem = entry.find(\"arxiv:primary_category\", ns)\n",
    "            if primary_cat_elem is not None:\n",
    "                primary_category = primary_cat_elem.attrib.get(\"term\", \"\")\n",
    "            # Fechas\n",
    "            published = entry.find(\"atom:published\", ns).text.strip() if entry.find(\"atom:published\", ns) is not None else \"\"\n",
    "            updated = entry.find(\"atom:updated\", ns).text.strip() if entry.find(\"atom:updated\", ns) is not None else \"\"\n",
    "            # Título y resumen\n",
    "            title = entry.find(\"atom:title\", ns).text.strip() if entry.find(\"atom:title\", ns) is not None else \"\"\n",
    "            summary = entry.find(\"atom:summary\", ns).text.strip() if entry.find(\"atom:summary\", ns) is not None else \"\"\n",
    "            # Autores (lista de diccionarios)\n",
    "            authors = []\n",
    "            for author in entry.findall(\"atom:author\", ns):\n",
    "                name = author.find(\"atom:name\", ns).text.strip() if author.find(\"atom:name\", ns) is not None else \"\"\n",
    "                authors.append({\"name\": name})\n",
    "            # Link del artículo (se toma el primero con rel=\"alternate\")\n",
    "            link_article = \"\"\n",
    "            for link in entry.findall(\"atom:link\", ns):\n",
    "                if link.attrib.get(\"rel\") == \"alternate\":\n",
    "                    link_article = link.attrib.get(\"href\", \"\")\n",
    "                    break\n",
    "            article = {\n",
    "                \"title\": title,\n",
    "                \"published\": published,\n",
    "                \"summary\": summary,\n",
    "                \"primary_category\": primary_category,\n",
    "                \"updated\": updated,\n",
    "                \"authors\": authors,\n",
    "                \"link_article\": link_article,\n",
    "                \"version\": version,\n",
    "            }\n",
    "            articles.append(article)\n",
    "        return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddabbe22-e36e-4295-a110-e5ae8600a0c5",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52cd71ff-4636-4370-b425-c9dff0038fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3ARAG%26id_list%3D%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=all:RAG&amp;id_list=&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/j/JCFc+b/wEUWgIEysf/YgADPTs</id>\\n  <updated>2025-02-25T00:00:00-05:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1579</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.18418v1</id>\\n    <updated>2025-02-25T18:14:06Z</updated>\\n    <published>2025-02-25T18:14:06Z</published>\\n    <title>Rank1: Test-Time Compute for Reranking in Information Retrieval</title>\\n    <summary>  We introduce Rank1, the first reranking model trained to take advantage of\\ntest-time compute. Rank1 demonstrates the applicability within retrieval of\\nusing a reasoning language model (i.e. OpenAI\\'s o1, Deepseek\\'s R1, etc.) for\\ndistillation in order to rapidly improve the performance of a smaller model. We\\ngather and open-source a dataset of more than 600,000 examples of R1 reasoning\\ntraces from queries and passages in MS MARCO. Models trained on this dataset\\nshow: (1) state-of-the-art performance on advanced reasoning and instruction\\nfollowing datasets; (2) work remarkably well out of distribution due to the\\nability to respond to user-input prompts; and (3) have explainable reasoning\\nchains that can be given to users or RAG-based systems. Further, we demonstrate\\nthat quantized versions of these models retain strong performance while using\\nless compute/memory. Overall, Rank1 shows that test-time compute allows for a\\nfundamentally new type of explainable and performant reranker model for search.\\n</summary>\\n    <author>\\n      <name>Orion Weller</name>\\n    </author>\\n    <author>\\n      <name>Kathryn Ricci</name>\\n    </author>\\n    <author>\\n      <name>Eugene Yang</name>\\n    </author>\\n    <author>\\n      <name>Andrew Yates</name>\\n    </author>\\n    <author>\\n      <name>Dawn Lawrie</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Van Durme</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2502.18418v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.18418v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.18139v1</id>\\n    <updated>2025-02-25T12:09:16Z</updated>\\n    <published>2025-02-25T12:09:16Z</published>\\n    <title>LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic\\n  Planning over Rewriting Augmented Searchers</title>\\n    <summary>  Retrieval-Augmented Generation (RAG) is a crucial method for mitigating\\nhallucinations in Large Language Models (LLMs) and integrating external\\nknowledge into their responses. Existing RAG methods typically employ query\\nrewriting to clarify the user intent and manage multi-hop logic, while using\\nhybrid retrieval to expand search scope. However, the tight coupling of query\\nrewriting to the dense retriever limits its compatibility with hybrid\\nretrieval, impeding further RAG performance improvements. To address this\\nchallenge, we introduce a high-level searcher that decomposes complex queries\\ninto atomic queries, independent of any retriever-specific optimizations.\\nAdditionally, to harness the strengths of sparse retrievers for precise keyword\\nretrieval, we have developed a new sparse searcher that employs Lucene syntax\\nto enhance retrieval accuracy.Alongside web and dense searchers, these\\ncomponents seamlessly collaborate within our proposed method,\\n\\\\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the\\nretrieval logic, while the low-level searchers (sparse, web, and dense) refine\\nthe queries for optimal retrieval. This approach enhances both the completeness\\nand accuracy of the retrieval process, overcoming challenges associated with\\ncurrent query rewriting techniques in hybrid retrieval scenarios. Empirical\\nexperiments conducted on five datasets, encompassing both single-hop and\\nmulti-hop question answering tasks, demonstrate the superior performance of\\nLevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the\\nstate-of-the-art proprietary model, GPT4o, underscoring its effectiveness and\\npotential impact on the RAG field.\\n</summary>\\n    <author>\\n      <name>Zhuocheng Zhang</name>\\n    </author>\\n    <author>\\n      <name>Yang Feng</name>\\n    </author>\\n    <author>\\n      <name>Min Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">First submit</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2502.18139v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.18139v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.18023v1</id>\\n    <updated>2025-02-25T09:32:08Z</updated>\\n    <published>2025-02-25T09:32:08Z</published>\\n    <title>Detecting Knowledge Boundary of Vision Large Language Models by\\n  Sampling-Based Inference</title>\\n    <summary>  Despite the advancements made in Visual Large Language Models (VLLMs), like\\ntext Large Language Models (LLMs), they have limitations in addressing\\nquestions that require real-time information or are knowledge-intensive.\\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\\neffective yet expensive way to enable models to answer queries beyond their\\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\\nmaintain, or even improve, the performance benefits provided by retrieval, we\\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\\nefficient use of techniques like RAG. Specifically, we propose a method with\\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\\nboundary identification. Experimental results on various types of Visual\\nQuestion Answering datasets show that our method successfully depicts a VLLM\\'s\\nknowledge boundary based on which we are able to reduce indiscriminate\\nretrieval while maintaining or improving the performance. In addition, we show\\nthat the knowledge boundary identified by our method for one VLLM can be used\\nas a surrogate boundary for other VLLMs. Code will be released at\\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary\\n</summary>\\n    <author>\\n      <name>Zhuo Chen</name>\\n    </author>\\n    <author>\\n      <name>Xinyu Wang</name>\\n    </author>\\n    <author>\\n      <name>Yong Jiang</name>\\n    </author>\\n    <author>\\n      <name>Zhen Zhang</name>\\n    </author>\\n    <author>\\n      <name>Xinyu Geng</name>\\n    </author>\\n    <author>\\n      <name>Pengjun Xie</name>\\n    </author>\\n    <author>\\n      <name>Fei Huang</name>\\n    </author>\\n    <author>\\n      <name>Kewei Tu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Under review</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2502.18023v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.18023v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.18017v1</id>\\n    <updated>2025-02-25T09:26:12Z</updated>\\n    <published>2025-02-25T09:26:12Z</published>\\n    <title>ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic\\n  Iterative Reasoning Agents</title>\\n    <summary>  Understanding information from visually rich documents remains a significant\\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\\nExisting benchmarks predominantly focus on image-based question answering (QA),\\noverlooking the fundamental challenges of efficient retrieval, comprehension,\\nand reasoning within dense visual documents. To bridge this gap, we introduce\\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\\ndocuments requiring complex reasoning. Based on it, we identify key limitations\\nin current RAG approaches: (i) purely visual retrieval methods struggle to\\neffectively integrate both textual and visual features, and (ii) previous\\napproaches often allocate insufficient reasoning tokens, limiting their\\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\\nmulti-agent RAG framework tailored for complex reasoning across visual\\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\\nto effectively handle multi-modal retrieval. To further elicit the model\\'s\\nreasoning capabilities, we introduce an iterative agent workflow incorporating\\nexploration, summarization, and reflection, providing a framework for\\ninvestigating test-time scaling in RAG domains. Extensive experiments on\\nViDoSeek validate the effectiveness and generalization of our approach.\\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\\nViDoSeek benchmark.\\n</summary>\\n    <author>\\n      <name>Qiuchen Wang</name>\\n    </author>\\n    <author>\\n      <name>Ruixue Ding</name>\\n    </author>\\n    <author>\\n      <name>Zehui Chen</name>\\n    </author>\\n    <author>\\n      <name>Weiqi Wu</name>\\n    </author>\\n    <author>\\n      <name>Shihang Wang</name>\\n    </author>\\n    <author>\\n      <name>Pengjun Xie</name>\\n    </author>\\n    <author>\\n      <name>Feng Zhao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2502.18017v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.18017v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.17888v1</id>\\n    <updated>2025-02-25T06:18:05Z</updated>\\n    <published>2025-02-25T06:18:05Z</published>\\n    <title>RankCoT: Refining Knowledge for Retrieval-Augmented Generation through\\n  Ranking Chain-of-Thoughts</title>\\n    <summary>  Retrieval-Augmented Generation (RAG) enhances the performance of Large\\nLanguage Models (LLMs) by incorporating external knowledge. However, LLMs still\\nencounter challenges in effectively utilizing the knowledge from retrieved\\ndocuments, often being misled by irrelevant or noisy information. To address\\nthis issue, we introduce RankCoT, a knowledge refinement method that\\nincorporates reranking signals in generating CoT-based summarization for\\nknowledge refinement based on given query and all retrieval documents. During\\ntraining, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates\\nbased on the query and individual documents. It then fine-tunes the LLM to\\ndirectly reproduce the best CoT from these candidate outputs based on all\\nretrieved documents, which requires LLM to filter out irrelevant documents\\nduring generating CoT-style summarization. Additionally, RankCoT incorporates a\\nself-reflection mechanism that further refines the CoT outputs, resulting in\\nhigher-quality training data. Our experiments demonstrate the effectiveness of\\nRankCoT, showing its superior performance over other knowledge refinement\\nmodels. Further analysis reveals that RankCoT can provide shorter but effective\\nrefinement results, enabling the generator to produce more accurate answers.\\nAll code and data are available at https://github.com/NEUIR/RankCoT.\\n</summary>\\n    <author>\\n      <name>Mingyan Wu</name>\\n    </author>\\n    <author>\\n      <name>Zhenghao Liu</name>\\n    </author>\\n    <author>\\n      <name>Yukun Yan</name>\\n    </author>\\n    <author>\\n      <name>Xinze Li</name>\\n    </author>\\n    <author>\\n      <name>Shi Yu</name>\\n    </author>\\n    <author>\\n      <name>Zheni Zeng</name>\\n    </author>\\n    <author>\\n      <name>Yu Gu</name>\\n    </author>\\n    <author>\\n      <name>Ge Yu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2502.17888v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.17888v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.17839v1</id>\\n    <updated>2025-02-25T04:38:38Z</updated>\\n    <published>2025-02-25T04:38:38Z</published>\\n    <title>Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented\\n  Generation</title>\\n    <summary>  We propose a simple, unsupervised method that injects pragmatic principles in\\nretrieval-augmented generation (RAG) frameworks such as Dense Passage\\nRetrieval~\\\\cite{karpukhin2020densepassageretrievalopendomain} to enhance the\\nutility of retrieved contexts. Our approach first identifies which sentences in\\na pool of documents retrieved by RAG are most relevant to the question at hand,\\ncover all the topics addressed in the input question and no more, and then\\nhighlights these sentences within their context, before they are provided to\\nthe LLM, without truncating or altering the context in any other way. We show\\nthat this simple idea brings consistent improvements in experiments on three\\nquestion answering tasks (ARC-Challenge, PubHealth and PopQA) using five\\ndifferent LLMs. It notably enhances relative accuracy by up to 19.7\\\\% on\\nPubHealth and 10\\\\% on ARC-Challenge compared to a conventional RAG system.\\n</summary>\\n    <author>\\n      <name>Haris Riaz</name>\\n    </author>\\n    <author>\\n      <name>Ellen Riloff</name>\\n    </author>\\n    <author>\\n      <name>Mihai Surdeanu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2502.17839v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.17839v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.17832v1</id>\\n    <updated>2025-02-25T04:23:59Z</updated>\\n    <published>2025-02-25T04:23:59Z</published>\\n    <title>MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning\\n  Attacks</title>\\n    <summary>  Multimodal large language models (MLLMs) equipped with Retrieval Augmented\\nGeneration (RAG) leverage both their rich parametric knowledge and the dynamic,\\nexternal knowledge to excel in tasks such as Question Answering. While RAG\\nenhances MLLMs by grounding responses in query-relevant external knowledge,\\nthis reliance poses a critical yet underexplored safety risk: knowledge\\npoisoning attacks, where misinformation or irrelevant knowledge is\\nintentionally injected into external knowledge bases to manipulate model\\noutputs to be incorrect and even harmful. To expose such vulnerabilities in\\nmultimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack\\nframework with two attack strategies: Localized Poisoning Attack (LPA), which\\ninjects query-specific misinformation in both text and images for targeted\\nmanipulation, and Globalized Poisoning Attack (GPA) to provide false guidance\\nduring MLLM generation to elicit nonsensical responses across all queries. We\\nevaluate our attacks across multiple tasks, models, and access settings,\\ndemonstrating that LPA successfully manipulates the MLLM to generate\\nattacker-controlled answers, with a success rate of up to 56% on MultiModalQA.\\nMoreover, GPA completely disrupts model generation to 0% accuracy with just a\\nsingle irrelevant knowledge injection. Our results highlight the urgent need\\nfor robust defenses against knowledge poisoning to safeguard multimodal RAG\\nframeworks.\\n</summary>\\n    <author>\\n      <name>Hyeonjeong Ha</name>\\n    </author>\\n    <author>\\n      <name>Qiusi Zhan</name>\\n    </author>\\n    <author>\\n      <name>Jeonghwan Kim</name>\\n    </author>\\n    <author>\\n      <name>Dimitrios Bralios</name>\\n    </author>\\n    <author>\\n      <name>Saikrishna Sanniboina</name>\\n    </author>\\n    <author>\\n      <name>Nanyun Peng</name>\\n    </author>\\n    <author>\\n      <name>Kai-wei Chang</name>\\n    </author>\\n    <author>\\n      <name>Daniel Kang</name>\\n    </author>\\n    <author>\\n      <name>Heng Ji</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Code is available at https://github.com/HyeonjeongHa/MM-PoisonRAG</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2502.17832v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.17832v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.17611v1</id>\\n    <updated>2025-02-24T19:58:23Z</updated>\\n    <published>2025-02-24T19:58:23Z</published>\\n    <title>Evaluating the Effect of Retrieval Augmentation on Social Biases</title>\\n    <summary>  Retrieval Augmented Generation (RAG) has gained popularity as a method for\\nconveniently incorporating novel facts that were not seen during the\\npre-training stage in Large Language Model (LLM)-based Natural Language\\nGeneration (NLG) systems. However, LLMs are known to encode significant levels\\nof unfair social biases. The modulation of these biases by RAG in NLG systems\\nis not well understood. In this paper, we systematically study the relationship\\nbetween the different components of a RAG system and the social biases\\npresented in the text generated across three languages (i.e. English, Japanese\\nand Chinese) and four social bias types (i.e. gender, race, age and religion).\\nSpecifically, using the Bias Question Answering (BBQ) benchmark datasets, we\\nevaluate the social biases in RAG responses from document collections with\\nvarying levels of stereotypical biases, employing multiple LLMs used as\\ngenerators. We find that the biases in document collections are often amplified\\nin the generated responses, even when the generating LLM exhibits a low-level\\nof bias. Our findings raise concerns about the use of RAG as a technique for\\ninjecting novel facts into NLG systems and call for careful evaluation of\\npotential social biases in RAG applications before their real-world deployment.\\n</summary>\\n    <author>\\n      <name>Tianhui Zhang</name>\\n    </author>\\n    <author>\\n      <name>Yi Zhou</name>\\n    </author>\\n    <author>\\n      <name>Danushka Bollegala</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2502.17611v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.17611v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.17390v1</id>\\n    <updated>2025-02-24T18:16:10Z</updated>\\n    <published>2025-02-24T18:16:10Z</published>\\n    <title>Mitigating Bias in RAG: Controlling the Embedder</title>\\n    <summary>  In retrieval augmented generation (RAG) systems, each individual component --\\nthe LLM, embedder, and corpus -- could introduce biases in the form of skews\\ntowards outputting certain perspectives or identities. In this work, we study\\nthe conflict between biases of each component and their relationship to the\\noverall bias of the RAG system, which we call bias conflict. Examining both\\ngender and political biases as case studies, we show that bias conflict can be\\ncharacterized through a linear relationship among components despite its\\ncomplexity in 6 different LLMs. Through comprehensive fine-tuning experiments\\ncreating 120 differently biased embedders, we demonstrate how to control bias\\nwhile maintaining utility and reveal the importance of reverse-biasing the\\nembedder to mitigate bias in the overall system. Additionally, we find that\\nLLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial\\nfactor to consider for debiasing. Our results underscore that a fair RAG system\\ncan be better achieved by carefully controlling the bias of the embedder rather\\nthan increasing its fairness.\\n</summary>\\n    <author>\\n      <name>Taeyoun Kim</name>\\n    </author>\\n    <author>\\n      <name>Jacob Springer</name>\\n    </author>\\n    <author>\\n      <name>Aditi Raghunathan</name>\\n    </author>\\n    <author>\\n      <name>Maarten Sap</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages (8 main), 12 figures, 7 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2502.17390v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.17390v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2502.17297v1</id>\\n    <updated>2025-02-24T16:25:25Z</updated>\\n    <published>2025-02-24T16:25:25Z</published>\\n    <title>Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts</title>\\n    <summary>  This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a\\nbenchmark designed to evaluate the effectiveness of Multi-modal Large Language\\nModels (MLLMs) in leveraging knowledge from multi-modal retrieval documents.\\nThe benchmark comprises four tasks: image captioning, multi-modal question\\nanswering, multi-modal fact verification, and image reranking. All tasks are\\nset in an open-domain setting, requiring RAG models to retrieve query-relevant\\ninformation from a multi-modal document collection and use it as input context\\nfor RAG modeling. To enhance the context utilization capabilities of MLLMs, we\\nalso introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an\\ninstruction tuning method that optimizes MLLMs within multi-modal contexts. Our\\nexperiments show that MM-RAIT improves the performance of RAG systems by\\nenabling them to effectively learn from multi-modal contexts. All data and code\\nare available at https://github.com/NEUIR/M2RAG.\\n</summary>\\n    <author>\\n      <name>Zhenghao Liu</name>\\n    </author>\\n    <author>\\n      <name>Xingsheng Zhu</name>\\n    </author>\\n    <author>\\n      <name>Tianshuo Zhou</name>\\n    </author>\\n    <author>\\n      <name>Xinyi Zhang</name>\\n    </author>\\n    <author>\\n      <name>Xiaoyuan Yi</name>\\n    </author>\\n    <author>\\n      <name>Yukun Yan</name>\\n    </author>\\n    <author>\\n      <name>Yu Gu</name>\\n    </author>\\n    <author>\\n      <name>Ge Yu</name>\\n    </author>\\n    <author>\\n      <name>Maosong Sun</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2502.17297v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2502.17297v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
